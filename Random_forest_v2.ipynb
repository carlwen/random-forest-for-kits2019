{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1afa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract feature from original image and sobel filtered image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.sparse import dok_matrix,coo_matrix,csr_matrix,vstack,save_npz\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib2 import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918b3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_dice(tp,fp,fn):\n",
    "    return (2*tp)/(2*tp+fp+fn)\n",
    "\n",
    "def kedney_or_tumor(x):\n",
    "    y = x.copy()\n",
    "    y[y==2] = 1\n",
    "    y[y==3] = 0\n",
    "    return y\n",
    "\n",
    "def tumor_only(y):\n",
    "    x = y.copy()\n",
    "    x[x==1] = 0\n",
    "    x[x==3] = 0\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7e42d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(876480949, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = np.load('data_train.npy')\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc50a1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  1.,  2.,  3.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(data_train[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84106421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (data_train[:,-1]==-1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9556b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867193466 7367037 1918904 967\n"
     ]
    }
   ],
   "source": [
    "# index_0 = np.arange(0,len(data_train),25)\n",
    "index_0 = data_train[:,-1]==0\n",
    "index_1 = data_train[:,-1]==1\n",
    "index_2 = data_train[:,-1]==2\n",
    "index_3 = data_train[:,-1]==3\n",
    "print(index_0.sum(),index_1.sum(),index_2.sum(),index_3.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b798e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = data_train[index_0,:]\n",
    "data_1 = data_train[index_1,:]\n",
    "data_2 = data_train[index_2,:]\n",
    "data_3 = data_train[index_3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_0.shape)\n",
    "# data_0 = np.unique(data_0,axis = 0)\n",
    "# print(data_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a871f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2275b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=0,\n",
    "                                bootstrap = True,\n",
    "                                max_samples = 0.25,\n",
    "                                min_samples_split = 10,\n",
    "                                min_samples_leaf = 5,\n",
    "                                class_weight = 'balanced',\n",
    "                                max_depth=30, \n",
    "                                random_state=456,\n",
    "                                n_jobs=8,\n",
    "#                                 max_features=2,\n",
    "#                                 criterion='entropy',\n",
    "                                verbose=2,\n",
    "#                                 ccp_alpha=0.00001,\n",
    "                                warm_start=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f94f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.6s remaining:   41.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.9s remaining:   41.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8building tree 8 of 8\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.5s remaining:   42.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8building tree 6 of 8\n",
      "\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.9s remaining:   41.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.4s remaining:   42.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.2s remaining:   42.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.2s remaining:   41.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8building tree 3 of 8\n",
      "building tree 4 of 8building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.1s remaining:   40.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.3s remaining:   42.1s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   26.1s remaining:   43.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.5s remaining:   42.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.0s remaining:   41.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.9s remaining:   41.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.4s remaining:   40.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.5s remaining:   40.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.6s remaining:   40.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.5s remaining:   40.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.6s remaining:   41.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.3s remaining:   42.2s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   29.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.5s remaining:   42.4s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   23.8s remaining:   39.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   26.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.5s remaining:   40.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   30.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   25.2s remaining:   42.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   28.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8building tree 8 of 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   24.3s remaining:   40.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   27.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:587: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn('class_weight presets \"balanced\" or '\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 8\n",
      "building tree 2 of 8\n",
      "building tree 3 of 8\n",
      "building tree 4 of 8\n",
      "building tree 5 of 8\n",
      "building tree 6 of 8\n",
      "building tree 7 of 8\n",
      "building tree 8 of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:   23.7s remaining:   39.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   25.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:   25.3s finished\n"
     ]
    }
   ],
   "source": [
    "slice_num = 25\n",
    "for i in range(slice_num):\n",
    "    print(i)\n",
    "    data_new_slice = np.concatenate((np.unique(data_0[np.arange(i,len(data_0),slice_num)],axis=0),\n",
    "                               data_1[np.arange(i,len(data_1),slice_num)],\n",
    "                               data_2[np.arange(i,len(data_2),slice_num)],\n",
    "                               data_3[np.arange(i,len(data_3),slice_num)]),0)\n",
    "\n",
    "    fea_datasets = data_new_slice[:,:5]\n",
    "    target_list = data_new_slice[:,5]\n",
    "    \n",
    "    forest.n_estimators = forest.n_estimators + 8\n",
    "    forest.fit(fea_datasets, target_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17b15ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RF_warmv2.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(forest, 'RF_warmv2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_list = target_list[~np.isnan(fea_datasets).any(axis=1)]\n",
    "\n",
    "\n",
    "# fea_datasets = fea_datasets[~np.isnan(fea_datasets).any(axis=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001875d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# y_true_temp = data_train[:,5]\n",
    "# # ypred = bst.predict(fea_datasets)\n",
    "# pred = forest.predict(data_train[:,:5])\n",
    "# # pred = np.argmax(ypred, axis=1)\n",
    "# accuracy = accuracy_score(y_true_temp, pred)\n",
    "\n",
    "# cm1 = confusion_matrix(kedney_or_tumor(y_true_temp), kedney_or_tumor(pred))\n",
    "# cm2 = confusion_matrix(tumor_only(y_true_temp), tumor_only(pred))\n",
    "# if cm1.shape[0]>1:\n",
    "#     dice1 = cal_dice(cm1[1,1],cm1[0,1],cm1[1,0])\n",
    "# if cm2.shape[0]>1:\n",
    "#     dice2 = cal_dice(cm2[1,1],cm2[0,1],cm2[1,0])\n",
    "# print(accuracy,dice1,dice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fdc5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = load ('RF_warmv2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1876810c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   15.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9928070508993869 0.6093422450341709 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9884416993946717 0.5490685615517572 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   14.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.9834444171452063 0.5235218680547093 0.025922758076189222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   13.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.9909047727441653 0.6082702446937948 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   13.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.9786022047346514 0.04265092468754324 0.000168584313229654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   20.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.9847816046517299 0.2969010813556862 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   19.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.9819751584427343 0.3827944826106672 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    9.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.9727406119356664 0.1493005457624992 0.010768807171046594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   21.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.9783503664395812 0.37769183730808004 0.09021312851611933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   10.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.9902569570904151 0.42591018844509193 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.9771982121749409 0.19460082218323688 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   17.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.9775993844760191 0.36124776068473285 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   19.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.983617252047878 0.4318322089659577 0.010833200573522384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   11.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.9741614528503525 0.24671033478893742 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.9860797925744667 0.041745000480933186 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   10.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.9716385667460493 0.46926561337476475 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 0.9800825065981713 0.4271045008430974 0.00010985992859104642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   17.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 0.975578231292517 0.3685029824633908 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   26.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 0.9934442942303009 0.6833319224583086 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   42.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.9903149083799427 0.5236775771374244 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   31.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.9891635605001301 0.5089269716498569 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 0.9912798836840155 0.6355274604187207 0.0784524449220849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   14.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 0.9892972505079035 0.5193228664152267 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 0.9884500647445459 0.6003901739857467 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   15.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 0.983812889739229 0.3623260822137614 0.05298249791641862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 0.9639070529941609 0.3460394351116695 0.004261423926791793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   21.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 0.9763479560272502 0.1499665661245087 0.026002632497918177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   19.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 0.9868802087204549 0.33730959809139294 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 0.9858474652055151 0.4627656477438137 0.18248826012007371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   17.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 0.988218770964268 0.4820722277925066 0.005599800895968144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.9284518401841756 0.5745614621583095 0.19697121546990468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    8.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 0.9618871455482848 0.24084050612009 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   19.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 0.9890577750099148 0.37843043558169454 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   20.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 0.9823186042777705 0.36955137534565563 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   20.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 0.9896423149307362 0.5969531225858287 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   21.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 0.9810042842741935 0.2602964099265161 0.0013694461351186854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   22.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 0.9756170584433532 0.34437034794059446 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   21.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 0.9770008607163762 0.047487733746883816 0.1176843216610945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    9.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 0.9770715182761583 0.596402327905473 0.27167899753382657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   16.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 0.9905361970923626 0.5412744444272134 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 0.9866207584556943 0.3936677177128369 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 0.9838350147028853 0.6621846033071992 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 0.9583310605552012 0.267941604934445 0.057869210625539104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   19.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 0.9803394506714067 0.3658435350355602 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   20.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 0.9805163840969693 0.4169243894635811 0.156198470993854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 0.9794130575518728 0.2590151688574561 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   19.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 0.9907349776030827 0.37445625595222776 0.07578712733351908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   21.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 0.984766615789217 0.3704670508210216 0.022367515485203028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   22.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 0.9837711266746878 0.0731214403821422 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   17.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 0.9892794652505403 0.5156955455224146 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   10.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.9774081867407345 0.5076301478207023 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    9.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 0.9888082137317653 0.7409340748690929 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   18.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 0.9936888347951043 0.7117100657014858 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   13.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 0.9942340924857659 0.562202349849962 0.014517506404782237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 0.9901098912265869 0.45479459736036054 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   22.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 0.9815235594432883 0.4744336700609349 0.15693985164000357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   23.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 0.9790338310741038 0.29890250105611643 0.20855323612884338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   13.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 0.9852670828997572 0.09694116351422043 0.004597986827389089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   22.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 0.9890649896754683 0.47353063260084377 0.08393144865973341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   19.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 0.9908186954259602 0.47967833258113485 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   21.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 0.9872167867130593 0.3822652992794012 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 0.9666308966354492 0.022385830132309006 0.02891271056661562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   20.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 0.9914093493919095 0.2358703328245666 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_val = np.load('data_val.npy')\n",
    "fea_val = data_val[:,:5]\n",
    "y_true = data_val[:,5]\n",
    "y_true[y_true==-1]=0\n",
    "id_val = data_val[:,6]\n",
    "\n",
    "result_all = np.empty([63,3])\n",
    "for i in range(63):\n",
    "    fea_val_temp = fea_val[id_val==i,:]\n",
    "    y_true_temp = y_true[id_val==i]\n",
    "#     ypred = bst.predict(fea_val_temp)\n",
    "    pred = forest.predict(fea_val_temp)\n",
    "#     pred = np.argmax(ypred, axis=1)\n",
    "    accuracy = accuracy_score(y_true_temp, pred)\n",
    "\n",
    "    cm1 = confusion_matrix(kedney_or_tumor(y_true_temp), kedney_or_tumor(pred))\n",
    "    cm2 = confusion_matrix(tumor_only(y_true_temp), tumor_only(pred))\n",
    "    if cm1.shape[0]>1:\n",
    "        dice1 = cal_dice(cm1[1,1],cm1[0,1],cm1[1,0])\n",
    "    if cm2.shape[0]>1:\n",
    "        dice2 = cal_dice(cm2[1,1],cm2[0,1],cm2[1,0])\n",
    "    print(i,accuracy,dice1,dice2)\n",
    "    result_all[i,:] = [accuracy,dice1,dice2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aa4d083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98223229, 0.40014099, 0.02992353])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_all.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae7ccb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('result_warm_v2',result_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075b07ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23357023, 0.29375179, 0.17601601, 0.15365656, 0.14300541])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7e3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb968ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a441b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078e6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "690de496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92325162, 0.20591091, 0.03725003])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.load('result_v2.npy')\n",
    "t.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe8e2a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93396437, 0.23405296, 0.04708714])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.load('result_v3.npy')\n",
    "t.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b569c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfe3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = y_true[~np.isnan(fea_val).any(axis=1)]\n",
    "# id_val = id_val[~np.isnan(fea_val).any(axis=1)]\n",
    "\n",
    "# fea_val = fea_val[~np.isnan(fea_val).any(axis=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78d68055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# enc.fit(target_list.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cc0f7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1., 2., 3.], dtype=float32)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcdb22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_onehot = enc.transform(target_list.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "954994e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_onehot = pd.DataFrame(target_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60856eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Saving data to binary file train.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Dataset at 0x7f986c53a520>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = lgb.Dataset(fea_datasets, label=target_list)\n",
    "\n",
    "train_data.save_binary('train.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83c60a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset('train.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07fd291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31, \n",
    "#          'min_data_in_leaf':\n",
    "         'objective': 'multiclass',\n",
    "         'num_class':4,\n",
    "         'boosting':'gbdt',\n",
    "         'tree_learner ':'data',\n",
    "         'num_threads':8,\n",
    "         'device_type':'cpu',\n",
    "#          'device_type':'gpu',\n",
    "#         'force_row_wise':True,\n",
    "#         'force_col_wise':True,\n",
    "        'bagging_fraction':0.75,\n",
    "        'verbose': 5,\n",
    "        'bagging_freq':10}\n",
    "# param['metric'] = 'multi_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4eb2cee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000402\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000026 seconds, init for row-wise cost 0.309516 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 33252429, number of used features: 5\n",
      "[LightGBM] [Debug] Use subset for bagging\n",
      "[LightGBM] [Info] Start training from score -0.327498\n",
      "[LightGBM] [Info] Start training from score -1.507155\n",
      "[LightGBM] [Info] Start training from score -2.852376\n",
      "[LightGBM] [Info] Start training from score -10.445440\n",
      "[LightGBM] [Debug] Re-bagging, using 24939836 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 24939658 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 24939314 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 24939106 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 24939463 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 24939924 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 24938969 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 24939566 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 24939301 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 24939581 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 11\n"
     ]
    }
   ],
   "source": [
    "num_round = 100\n",
    "bst = lgb.train(param, train_data, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.save_model('model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b3d89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8671987841850591 0.7946169518805983 0.25499198541406\n"
     ]
    }
   ],
   "source": [
    "y_true_temp = target_list\n",
    "ypred = bst.predict(fea_datasets)\n",
    "# pred = forest.predict(fea_datasets)\n",
    "pred = np.argmax(ypred, axis=1)\n",
    "accuracy = accuracy_score(y_true_temp, pred)\n",
    "\n",
    "cm1 = confusion_matrix(kedney_or_tumor(y_true_temp), kedney_or_tumor(pred))\n",
    "cm2 = confusion_matrix(tumor_only(y_true_temp), tumor_only(pred))\n",
    "if cm1.shape[0]>1:\n",
    "    dice1 = cal_dice(cm1[1,1],cm1[0,1],cm1[1,0])\n",
    "if cm2.shape[0]>1:\n",
    "    dice2 = cal_dice(cm2[1,1],cm2[0,1],cm2[1,0])\n",
    "print(accuracy,dice1,dice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b907ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.980937262341868 0.4115341990699253 0.0\n",
      "1 0.9661223188211204 0.34219762694999695 0.0\n",
      "2 0.9545979544841408 0.32969170460444275 0.030787943289368656\n",
      "3 0.9666676731826966 0.33378370485739317 0.0\n",
      "4 0.9591026599895942 0.14914337785954593 0.0004139072847682119\n",
      "5 0.9377257221075052 0.13536389879981242 0.0\n",
      "6 0.9477721819827977 0.2017217290185971 0.0\n",
      "7 0.9621853150044727 0.3841732895810019 0.01110994928634914\n",
      "8 0.9564550679630593 0.42907726979273525 0.14166777879294945\n",
      "9 0.9753137104000462 0.2969467974101894 0.0\n",
      "10 0.9568033392434988 0.2301704966641957 0.0\n",
      "11 0.945257048679019 0.22073542262003165 0.006825676815706726\n",
      "12 0.9589984431272509 0.30074913960584837 0.05104251938171137\n",
      "13 0.9411085490637062 0.1580529039883948 0.0022640732736441288\n",
      "14 0.962994022746488 0.06586075393147069 0.0\n",
      "15 0.9246585062467143 0.26615306467998645 0.0\n",
      "16 0.9413917060396833 0.2272559924992514 0.0001679590179996081\n",
      "17 0.9360976473922903 0.20943369704636902 0.00010535187526337969\n",
      "18 0.9792753489826643 0.4474235363811496 0.0\n",
      "19 0.9761899378495708 0.3875305583356577 0.0\n",
      "20 0.9640199976830776 0.251927108701637 0.0\n",
      "21 0.9810107431347995 0.5175979023215161 0.07364897178383549\n",
      "22 0.9598556888286012 0.2566932984315784 0.0\n",
      "23 0.9595576062548149 0.33678976966667384 0.0\n",
      "24 0.9540729520975056 0.24610247635343885 0.055480544985884374\n",
      "25 0.9324481018254158 0.40336228374508115 0.07653638539019458\n",
      "26 0.955029474708962 0.20532526317532954 0.059154529018627985\n",
      "27 0.9520209777296714 0.17657075470522093 0.0\n",
      "28 0.964146513844628 0.34261315034346146 0.2532561505065123\n",
      "29 0.9641443433991244 0.27408808430154014 0.02147763552517011\n",
      "30 0.8986109642063858 0.6222283486274665 0.3353891364200495\n",
      "31 0.9099797298785997 0.16921264605686268 0.0\n",
      "32 0.9686704755767069 0.21422446832006029 0.000168584313229654\n",
      "33 0.9481232927939646 0.2012714772589115 0.0\n",
      "34 0.9652232984846514 0.3497375567815084 0.00010017530678687704\n",
      "35 0.9611138096709158 0.254316242977614 0.00955167369417434\n",
      "36 0.938889899437435 0.22749260028454155 0.00037576326914044153\n",
      "37 0.957671579287526 0.09336733572229428 0.24068558290223127\n",
      "38 0.9377143234846983 0.4109178774679681 0.2709581424488038\n",
      "39 0.9668265182412142 0.27959059972637373 6.528906734567296e-05\n",
      "40 0.9671315985919351 0.25227243860100507 0.0\n",
      "41 0.9535366989838996 0.4250248721686217 0.0\n",
      "42 0.9171095087442516 0.2393931486563076 0.11923443572633277\n",
      "43 0.9436211755604266 0.1818398984120598 0.00022559359314195478\n",
      "44 0.952529637300013 0.26803628139357116 0.202679706601467\n",
      "45 0.9490378632730534 0.2717677224879022 0.0015888147442008262\n",
      "46 0.9803271179516779 0.3652588971418679 0.08807095690227797\n",
      "47 0.953129572873309 0.22577240990166392 0.03413955771275871\n",
      "48 0.9644885901729969 0.11419140014010037 9.232331625351982e-05\n",
      "49 0.9662063880773233 0.2928135635496014 0.0\n",
      "50 0.9300311421597509 0.28448536113419226 0.0003819709702062643\n",
      "51 0.973626504427385 0.5796280265470476 0.0018086453246518358\n",
      "52 0.9790232768445568 0.46703485263250305 8.377314233056883e-05\n",
      "53 0.9850844593538871 0.40579023805013575 0.04448155457719751\n",
      "54 0.981307618203694 0.460457915990852 7.447680047665153e-05\n",
      "55 0.9571740252666493 0.36438811887751194 0.30879867515114395\n",
      "56 0.9626183483174614 0.3954345221088363 0.3258334151681272\n",
      "57 0.9740212018730489 0.3266187933468361 0.04840953352499305\n",
      "58 0.9798261342758194 0.5031388592396012 0.09060118543607112\n",
      "59 0.9772604397593087 0.30442442284909926 0.0\n",
      "60 0.9650121841668835 0.21672071593380096 0.0021536252692031586\n",
      "61 0.9574141519250781 0.07970335551285512 0.01886593961496651\n",
      "62 0.9829436906542663 0.3446376567961909 0.0017025333696540452\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_val = np.load('data_val.npy')\n",
    "fea_val = data_val[:,:5]\n",
    "y_true = data_val[:,5]\n",
    "y_true[y_true==-1]=0\n",
    "id_val = data_val[:,6]\n",
    "\n",
    "result_all = np.empty([63,3])\n",
    "for i in range(63):\n",
    "    fea_val_temp = fea_val[id_val==i,:]\n",
    "    y_true_temp = y_true[id_val==i]\n",
    "    ypred = bst.predict(fea_val_temp)\n",
    "#     pred = forest.predict(fea_val_temp)\n",
    "    pred = np.argmax(ypred, axis=1)\n",
    "    accuracy = accuracy_score(y_true_temp, pred)\n",
    "\n",
    "    cm1 = confusion_matrix(kedney_or_tumor(y_true_temp), kedney_or_tumor(pred))\n",
    "    cm2 = confusion_matrix(tumor_only(y_true_temp), tumor_only(pred))\n",
    "    if cm1.shape[0]>1:\n",
    "        dice1 = cal_dice(cm1[1,1],cm1[0,1],cm1[1,0])\n",
    "    if cm2.shape[0]>1:\n",
    "        dice2 = cal_dice(cm2[1,1],cm2[0,1],cm2[1,0])\n",
    "    print(i,accuracy,dice1,dice2)\n",
    "    result_all[i,:] = [accuracy,dice1,dice2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4075cc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92044728, 0.20320874, 0.03876046])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_all.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46251367",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true[~np.isnan(fea_val).any(axis=1)]\n",
    "id_val = id_val[~np.isnan(fea_val).any(axis=1)]\n",
    "\n",
    "fea_val = fea_val[~np.isnan(fea_val).any(axis=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all = np.empty([63,3])\n",
    "for i in range(63):\n",
    "    fea_val_temp = fea_val[id_val==i,:]\n",
    "    y_true_temp = y_true[id_val==i]\n",
    "#     ypred = bst.predict(fea_val_temp)\n",
    "    pred = forest.predict(fea_val_temp)\n",
    "#     pred = np.argmax(ypred, axis=1)\n",
    "    accuracy = accuracy_score(y_true_temp, pred)\n",
    "\n",
    "    cm1 = confusion_matrix(kedney_or_tumor(y_true_temp), kedney_or_tumor(pred))\n",
    "    cm2 = confusion_matrix(tumor_only(y_true_temp), tumor_only(pred))\n",
    "    if cm1.shape[0]>1:\n",
    "        dice1 = cal_dice(cm1[1,1],cm1[0,1],cm1[1,0])\n",
    "    if cm2.shape[0]>1:\n",
    "        dice2 = cal_dice(cm2[1,1],cm2[0,1],cm2[1,0])\n",
    "    print(i,accuracy,dice1,dice2)\n",
    "    result_all[i,:] = [accuracy,dice1,dice2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d42ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b375143",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('result_v2',result_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy,dice1,dice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aabdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vali = np.arange(int(np.round(0.1*len(data))))\n",
    "data_vali = data[index_vali,:]\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6adec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique((data_vali[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bbf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = bst.predict(data_vali[:,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93cc9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aae419",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(ypred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = data_vali[:,-1]\n",
    "\n",
    "accuracy = accuracy_score(y_true, pred)\n",
    "\n",
    "cm1 = confusion_matrix(kedney_or_tumor(y_true), kedney_or_tumor(pred))\n",
    "cm2 = confusion_matrix(tumor_only(y_true), tumor_only(pred))\n",
    "if cm1.shape[0]>1:\n",
    "    dice1 = cal_dice(cm1[1,1],cm1[0,1],cm1[1,0])\n",
    "if cm2.shape[0]>1:\n",
    "    dice2 = cal_dice(cm2[1,1],cm2[0,1],cm2[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy,dice1,dice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy,dice1,dice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2464663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load ('RF_warmv2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e04b85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23357023, 0.29375179, 0.17601601, 0.15365656, 0.14300541])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16577ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(data_vali[:,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770be20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_true, pred)\n",
    "\n",
    "cm1 = confusion_matrix(kedney_or_tumor(y_true), kedney_or_tumor(pred))\n",
    "cm2 = confusion_matrix(tumor_only(y_true), tumor_only(pred))\n",
    "if cm1.shape[0]>1:\n",
    "    dice1 = cal_dice(cm1[1,1],cm1[0,1],cm1[1,0])\n",
    "if cm2.shape[0]>1:\n",
    "    dice2 = cal_dice(cm2[1,1],cm2[0,1],cm2[1,0])\n",
    "print(accuracy,dice1,dice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce5f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f2583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273820f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3c46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2557f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900128bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(fea_datasets, target_list, test_size = 0.4, random_state = 0)\n",
    "Val_num = int(np.round(0.9*len(data)))\n",
    "\n",
    "x_train = fea_datasets[Val_num:,:]\n",
    "x_test = fea_datasets[:Val_num,:]\n",
    "y_train = target_list[Val_num:]\n",
    "y_test = target_list[:Val_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0226629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest = RandomForestClassifier(n_estimators=4, random_state=456,n_jobs=4,max_features=2,criterion='entropy',warm_start=True)\n",
    "forest = RandomForestClassifier(n_estimators=200,max_depth=10, random_state=456,n_jobs=4,max_features=2,criterion='entropy',warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95322f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[np.isnan(x_train)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98669fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[~np.isnan(x_train).any(axis=1)]\n",
    "x_train = x_train[~np.isnan(x_train).any(axis=1), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = forest.fit(x_train, y_train)\n",
    "dump(forest, 'RF_v2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8906d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368978cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)\n",
    "data.columns = ['original','mean','mean_sobel','square','label']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f887e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a40ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['label']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['label']==-1,['label']]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26418fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[2871179,'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "Val_num = int(np.round(0.3*len(data)))\n",
    "Val_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52267e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[['original','mean','mean_sobel','square']][Val_num:]\n",
    "y_train = data['label'][Val_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64569e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = csr_matrix(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(fea_datasets, target_list, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = data[['original','mean','mean_sobel','square']][:Val_num]\n",
    "y_val = data['label'][:Val_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd9a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef76264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a2c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t=0\n",
    "dice_best = 0\n",
    "slice = 248*248\n",
    "for i in [182]:\n",
    "#     t = t + 1\n",
    "#     forest.set_params(n_estimators=t)\n",
    "#     train_slice = X_train[slice*(i):slice*(i+1)]\n",
    "    train_slice = data_cpu[['original','mean','mean_sobel','square']][Val_num:Val_num+slice*(i+1)]\n",
    "#     y_slice = y_train[slice*(i):slice*(i+1)]\n",
    "    y_slice = data_cpu['label'][Val_num:Val_num+slice*(i+1)]\n",
    "    forest = forest.fit(train_slice, y_slice)\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "#     if np.mod(i,100)== 0:\n",
    "#         print(i)\n",
    "#         choose = np.random.randint(0, high=63, size=1, dtype='l')[0]\n",
    "#         val_slice = data_cpu[['original','mean','mean_sobel','square']][choose*128*248*248:(choose+1)*128*248*248]\n",
    "#         output_val = forest.predict(val_slice)\n",
    "#         y_true = data_cpu['label'][choose*128*248*248:(choose+1)*128*248*248]\n",
    "        \n",
    "#         accuracy = accuracy_score(y_true, output_val)\n",
    "\n",
    "#         cm1 = confusion_matrix(kedney_or_tumor(y_true), kedney_or_tumor(output_val))\n",
    "#         cm2 = confusion_matrix(tumor_only(y_true), tumor_only(output_val))\n",
    "#         if cm1.shape[0]>1:\n",
    "#             dice1 = cal_dice(cm1[1,1],cm1[0,1],cm1[1,0])\n",
    "#         if cm2.shape[0]>1:\n",
    "#             dice2 = cal_dice(cm2[1,1],cm2[0,1],cm2[1,0])\n",
    "#         if (dice1+dice2)/2 > dice_best:\n",
    "#             dice_best = (dice1+dice2)/2\n",
    "#             dump(forest, 'RF_v1.joblib') \n",
    "#             print('save model')\n",
    "#         print(choose)\n",
    "#         print(accuracy,dice1,dice2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
